{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53e698a-2c23-41ea-9fd6-16e3f62d2c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/da/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/da/.ivy2/jars\n",
      "ai.eto#rikai_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9401266c-2a3c-47ee-875f-c8f5bc453be0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound ai.eto#rikai_2.12;0.0.12 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8-1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.12;12.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in spark-list\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.13.2 in central\n",
      "\tfound io.circe#circe-core_2.12;0.12.3 in central\n",
      "\tfound io.circe#circe-numbers_2.12;0.12.3 in central\n",
      "\tfound org.typelevel#cats-core_2.12;2.0.0 in central\n",
      "\tfound org.typelevel#cats-macros_2.12;2.0.0 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.0.0 in central\n",
      "\tfound io.circe#circe-generic_2.12;0.12.3 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.3 in spark-list\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in spark-list\n",
      "\tfound io.circe#circe-parser_2.12;0.12.3 in central\n",
      "\tfound io.circe#circe-jawn_2.12;0.12.3 in central\n",
      "\tfound org.typelevel#jawn-parser_2.12;0.14.2 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.13.0 in central\n",
      ":: resolution report :: resolve 4733ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tai.eto#rikai_2.12;0.0.12 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.3 from spark-list in [default]\n",
      "\tio.circe#circe-core_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-generic_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-jawn_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-numbers_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-parser_2.12;0.12.3 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8-1 from local-m2-cache in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.13.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.12;12.0 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.13.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from spark-list in [default]\n",
      "\torg.typelevel#cats-core_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#cats-macros_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#jawn-parser_2.12;0.14.2 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from spark-list in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   1   |   1   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9401266c-2a3c-47ee-875f-c8f5bc453be0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/9ms)\n",
      "21/10/28 10:05:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/28 10:06:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:78)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:589)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1000)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "21/10/28 10:06:07 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/10/28 10:06:17 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/10/28 10:06:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:78)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:589)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1000)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:524)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "2021-10-28 10:06:24,586 INFO Rikai (callback_service.py:54): Spark callback server started\n",
      "2021-10-28 10:06:24,592 INFO Rikai (callback_service.py:113): Rikai Python CallbackService is registered to SparkSession\n"
     ]
    }
   ],
   "source": [
    "from example import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8b73bd-5f66-4a0b-a533-36f8e470997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     950|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/tmp/rikai_example/elephants_dream\")\n",
    "df.createOrReplaceTempView(\"elephants_dream\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select count(*) from elephants_dream\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b269c5-7364-4abe-9130-ee424d850d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 10:06:29,594 INFO Rikai (mlflow_registry.py:223): Resolving model mlflow_yolov5_m from mlflow:///yolov5s-model\n",
      "2021-10-28 10:06:30,738 INFO Rikai (base.py:207): Created model inference pandas_udf with name mlflow_yolov5_m_baf9faf4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_tracking_uri = \"sqlite:///mlruns.db\"\n",
    "registered_model_name = f\"yolov5s-model\"\n",
    "\n",
    "spark.conf.set(\"rikai.sql.ml.registry.mlflow.tracking_uri\", mlflow_tracking_uri)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE MODEL mlflow_yolov5_m\n",
    "    PREPROCESSOR 'rikai.contrib.torch.transforms.yolov5.pre_processing'\n",
    "    POSTPROCESSOR 'rikai.contrib.torch.transforms.yolov5.post_processing'\n",
    "    USING 'mlflow:///{registered_model_name}';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20af65ab-bb47-4dab-81f4-27248bb5dfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.inspect_bounding_boxes(img: rikai.types.vision.Image, boxes, labels)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Bounding Boxes on an Image\n",
    "from rikai.types.vision import Image\n",
    "from rikai.spark.types.vision import ImageType\n",
    "from rikai.types.geometry import Box2d\n",
    "from pyspark.sql.functions import udf\n",
    "from PIL import ImageDraw\n",
    "\n",
    "@udf(returnType=ImageType())\n",
    "def inspect_bounding_boxes(img: Image, boxes, labels):\n",
    "    pil_image = img.to_pil()\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    for i in range(len(labels)):\n",
    "        box = boxes[i]\n",
    "        label = labels[i]\n",
    "        bbox = Box2d(box[0], box[1], box[2], box[3])\n",
    "        draw.rectangle(bbox.to_numpy().tolist(), outline=\"green\", width=2)\n",
    "        draw.text([bbox.xmin + 5, bbox.ymin - 10], str(label), fill=\"red\")\n",
    "    return Image.from_pil(pil_image)\n",
    "\n",
    "spark.udf.register(\"inspect_bbox\", inspect_bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3643288d-aa35-4ade-8cf8-b5588d2ac0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.frame(path: str, frame_id: int)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from rikai.types.vision import Image\n",
    "from pyspark.sql.functions import udf\n",
    "from rikai.spark.types.vision import ImageType\n",
    "\n",
    "\n",
    "@udf(returnType=ImageType())\n",
    "def frame(path: str, frame_id: int): \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frame_cnt = 0\n",
    "    img0 = None\n",
    "    while frame_cnt <= frame_id:\n",
    "        ret_val, img0 = cap.read()\n",
    "        if not ret_val:\n",
    "            print(f\"{frame_id} out of bound\")\n",
    "            return\n",
    "        frame_cnt = frame_cnt + 1\n",
    "    cap.release()\n",
    "    img = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "    return Image.from_array(img)\n",
    "\n",
    "spark.udf.register('frame', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9e2e1d-0be4-4afc-bc43-dbcbd21df516",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = spark.sql(\"\"\"\n",
    "from (\n",
    "    from (\n",
    "        select\n",
    "            frame_id,\n",
    "            ML_PREDICT(mlflow_yolov5_m, image) as pred \n",
    "        from elephants_dream\n",
    "    )\n",
    "    select\n",
    "        frame('elephants_dream.mp4', frame_id) as image,\n",
    "        pred\n",
    "    where\n",
    "        size(pred.label_ids) > 1\n",
    ")\n",
    "select\n",
    "    image,\n",
    "    pred,\n",
    "    inspect_bbox(image, pred.boxes, pred.label_ids) as bbox\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf4afd-9157-4a95-959f-75135b63eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "rows = predicts.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcc8c1-0b82-49fa-b219-3780f9269910",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0].image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e22c1-e8d0-4d93-99a0-445542a3c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0].bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7385c-d393-49b8-8306-a458f9f9ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0].pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fab95c0-cef7-4b37-b05d-2fa332267c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| label|\n",
      "+---+------+\n",
      "| 74|remote|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from parquet.`/tmp/rikai_example/coco_labels`\n",
    "where id = 74\n",
    "order by id asc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98026c83-f6a3-4b03-86ec-83fba572c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae537f2a-6166-4ba3-95aa-c36cf19d47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|frame_id|               image|\n",
      "+--------+--------------------+\n",
      "|     378|[63 6E 75 6D 70 7...|\n",
      "|     392|[63 6E 75 6D 70 7...|\n",
      "|      51|[63 6E 75 6D 70 7...|\n",
      "|     425|[63 6E 75 6D 70 7...|\n",
      "|     407|[63 6E 75 6D 70 7...|\n",
      "|      64|[63 6E 75 6D 70 7...|\n",
      "|      38|[63 6E 75 6D 70 7...|\n",
      "|      31|[63 6E 75 6D 70 7...|\n",
      "|     477|[63 6E 75 6D 70 7...|\n",
      "|     249|[63 6E 75 6D 70 7...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from elephants_dream\n",
    "\"\"\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed395d-4bbb-426d-b72a-08a01a87faf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
