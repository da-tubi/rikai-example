{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53e698a-2c23-41ea-9fd6-16e3f62d2c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/da/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/da/.ivy2/jars\n",
      "ai.eto#rikai_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9bb052d4-440d-4d8d-adc2-b9edc8132286;1.0\n",
      "\tconfs: [default]\n",
      "\tfound ai.eto#rikai_2.12;0.0.13-SNAPSHOT in local-ivy-cache\n",
      "\tfound org.antlr#antlr4-runtime;4.8-1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.12;12.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in spark-list\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.13.2 in central\n",
      "\tfound io.circe#circe-core_2.12;0.12.3 in central\n",
      "\tfound io.circe#circe-numbers_2.12;0.12.3 in central\n",
      "\tfound org.typelevel#cats-core_2.12;2.0.0 in central\n",
      "\tfound org.typelevel#cats-macros_2.12;2.0.0 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.0.0 in central\n",
      "\tfound io.circe#circe-generic_2.12;0.12.3 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.3 in spark-list\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in spark-list\n",
      "\tfound io.circe#circe-parser_2.12;0.12.3 in central\n",
      "\tfound io.circe#circe-jawn_2.12;0.12.3 in central\n",
      "\tfound org.typelevel#jawn-parser_2.12;0.14.2 in central\n",
      "\tfound org.bytedeco#javacv-platform;1.5.6 in central\n",
      "\tfound org.bytedeco#javacv;1.5.6 in central\n",
      "\tfound org.bytedeco#javacpp;1.5.6 in central\n",
      "\tfound org.bytedeco#openblas;0.3.17-1.5.6 in central\n",
      "\tfound org.bytedeco#opencv;4.5.3-1.5.6 in central\n",
      "\tfound org.bytedeco#ffmpeg;4.4-1.5.6 in central\n",
      "\tfound org.bytedeco#flycapture;2.13.3.31-1.5.6 in central\n",
      "\tfound org.bytedeco#libdc1394;2.2.6-1.5.6 in central\n",
      "\tfound org.bytedeco#libfreenect;0.5.7-1.5.6 in central\n",
      "\tfound org.bytedeco#libfreenect2;0.2.0-1.5.6 in central\n",
      "\tfound org.bytedeco#librealsense;1.12.4-1.5.6 in central\n",
      "\tfound org.bytedeco#librealsense2;2.44.0-1.5.6 in central\n",
      "\tfound org.bytedeco#videoinput;0.200-1.5.6 in central\n",
      "\tfound org.bytedeco#artoolkitplus;2.3.1-1.5.6 in central\n",
      "\tfound org.bytedeco#flandmark;1.07-1.5.6 in central\n",
      "\tfound org.bytedeco#leptonica;1.81.1-1.5.6 in central\n",
      "\tfound org.bytedeco#tesseract;4.1.1-1.5.6 in central\n",
      "\tfound org.bytedeco#openblas-platform;0.3.17-1.5.6 in central\n",
      "\tfound org.bytedeco#javacpp-platform;1.5.6 in central\n",
      "\tfound org.bytedeco#opencv-platform;4.5.3-1.5.6 in central\n",
      "\tfound org.bytedeco#ffmpeg-platform;4.4-1.5.6 in central\n",
      "\tfound org.bytedeco#flycapture-platform;2.13.3.31-1.5.6 in central\n",
      "\tfound org.bytedeco#libdc1394-platform;2.2.6-1.5.6 in central\n",
      "\tfound org.bytedeco#libfreenect-platform;0.5.7-1.5.6 in central\n",
      "\tfound org.bytedeco#libfreenect2-platform;0.2.0-1.5.6 in central\n",
      "\tfound org.bytedeco#librealsense-platform;1.12.4-1.5.6 in central\n",
      "\tfound org.bytedeco#librealsense2-platform;2.44.0-1.5.6 in central\n",
      "\tfound org.bytedeco#videoinput-platform;0.200-1.5.6 in central\n",
      "\tfound org.bytedeco#artoolkitplus-platform;2.3.1-1.5.6 in central\n",
      "\tfound org.bytedeco#flandmark-platform;1.07-1.5.6 in central\n",
      "\tfound org.bytedeco#leptonica-platform;1.81.1-1.5.6 in central\n",
      "\tfound org.bytedeco#tesseract-platform;4.1.1-1.5.6 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.13.0 in central\n",
      "downloading /Users/da/.ivy2/local/ai.eto/rikai_2.12/0.0.13-SNAPSHOT/jars/rikai_2.12.jar ...\n",
      "\t[SUCCESSFUL ] ai.eto#rikai_2.12;0.0.13-SNAPSHOT!rikai_2.12.jar (2ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/javacv-platform/1.5.6/javacv-platform-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#javacv-platform;1.5.6!javacv-platform.jar (281ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/javacv/1.5.6/javacv-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#javacv;1.5.6!javacv.jar (1417ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/openblas-platform/0.3.17-1.5.6/openblas-platform-0.3.17-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#openblas-platform;0.3.17-1.5.6!openblas-platform.jar (1202ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/opencv-platform/4.5.3-1.5.6/opencv-platform-4.5.3-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#opencv-platform;4.5.3-1.5.6!opencv-platform.jar (406ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/ffmpeg-platform/4.4-1.5.6/ffmpeg-platform-4.4-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#ffmpeg-platform;4.4-1.5.6!ffmpeg-platform.jar (299ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/flycapture-platform/2.13.3.31-1.5.6/flycapture-platform-2.13.3.31-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#flycapture-platform;2.13.3.31-1.5.6!flycapture-platform.jar (318ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/libdc1394-platform/2.2.6-1.5.6/libdc1394-platform-2.2.6-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#libdc1394-platform;2.2.6-1.5.6!libdc1394-platform.jar (718ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/libfreenect-platform/0.5.7-1.5.6/libfreenect-platform-0.5.7-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#libfreenect-platform;0.5.7-1.5.6!libfreenect-platform.jar (303ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/libfreenect2-platform/0.2.0-1.5.6/libfreenect2-platform-0.2.0-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#libfreenect2-platform;0.2.0-1.5.6!libfreenect2-platform.jar (306ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/librealsense-platform/1.12.4-1.5.6/librealsense-platform-1.12.4-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#librealsense-platform;1.12.4-1.5.6!librealsense-platform.jar (326ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/librealsense2-platform/2.44.0-1.5.6/librealsense2-platform-2.44.0-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#librealsense2-platform;2.44.0-1.5.6!librealsense2-platform.jar (315ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/videoinput-platform/0.200-1.5.6/videoinput-platform-0.200-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#videoinput-platform;0.200-1.5.6!videoinput-platform.jar (274ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/artoolkitplus-platform/2.3.1-1.5.6/artoolkitplus-platform-2.3.1-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#artoolkitplus-platform;2.3.1-1.5.6!artoolkitplus-platform.jar (413ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/flandmark-platform/1.07-1.5.6/flandmark-platform-1.07-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#flandmark-platform;1.07-1.5.6!flandmark-platform.jar (409ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/leptonica-platform/1.81.1-1.5.6/leptonica-platform-1.81.1-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#leptonica-platform;1.81.1-1.5.6!leptonica-platform.jar (307ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/tesseract-platform/4.1.1-1.5.6/tesseract-platform-4.1.1-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#tesseract-platform;4.1.1-1.5.6!tesseract-platform.jar (410ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/javacpp/1.5.6/javacpp-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#javacpp;1.5.6!javacpp.jar (3030ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/openblas/0.3.17-1.5.6/openblas-0.3.17-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#openblas;0.3.17-1.5.6!openblas.jar (31215ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/opencv/4.5.3-1.5.6/opencv-4.5.3-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#opencv;4.5.3-1.5.6!opencv.jar (17700ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/ffmpeg/4.4-1.5.6/ffmpeg-4.4-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#ffmpeg;4.4-1.5.6!ffmpeg.jar (7284ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/flycapture/2.13.3.31-1.5.6/flycapture-2.13.3.31-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#flycapture;2.13.3.31-1.5.6!flycapture.jar (3393ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/libdc1394/2.2.6-1.5.6/libdc1394-2.2.6-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#libdc1394;2.2.6-1.5.6!libdc1394.jar (1000ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/libfreenect/0.5.7-1.5.6/libfreenect-0.5.7-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#libfreenect;0.5.7-1.5.6!libfreenect.jar (1323ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/libfreenect2/0.2.0-1.5.6/libfreenect2-0.2.0-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#libfreenect2;0.2.0-1.5.6!libfreenect2.jar (1335ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/librealsense/1.12.4-1.5.6/librealsense-1.12.4-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#librealsense;1.12.4-1.5.6!librealsense.jar (1276ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/librealsense2/2.44.0-1.5.6/librealsense2-2.44.0-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#librealsense2;2.44.0-1.5.6!librealsense2.jar (14974ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/videoinput/0.200-1.5.6/videoinput-0.200-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#videoinput;0.200-1.5.6!videoinput.jar (1044ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/artoolkitplus/2.3.1-1.5.6/artoolkitplus-2.3.1-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#artoolkitplus;2.3.1-1.5.6!artoolkitplus.jar (1099ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/flandmark/1.07-1.5.6/flandmark-1.07-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#flandmark;1.07-1.5.6!flandmark.jar (1101ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/leptonica/1.81.1-1.5.6/leptonica-1.81.1-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#leptonica;1.81.1-1.5.6!leptonica.jar (7542ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/tesseract/4.1.1-1.5.6/tesseract-4.1.1-1.5.6-windows-x86_64.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#tesseract;4.1.1-1.5.6!tesseract.jar (2377ms)\n",
      "downloading https://repo1.maven.org/maven2/org/bytedeco/javacpp-platform/1.5.6/javacpp-platform-1.5.6.jar ...\n",
      "\t[SUCCESSFUL ] org.bytedeco#javacpp-platform;1.5.6!javacpp-platform.jar (285ms)\n",
      ":: resolution report :: resolve 70018ms :: artifacts dl 103709ms\n",
      "\t:: modules in use:\n",
      "\tai.eto#rikai_2.12;0.0.13-SNAPSHOT from local-ivy-cache in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.3 from spark-list in [default]\n",
      "\tio.circe#circe-core_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-generic_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-jawn_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-numbers_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-parser_2.12;0.12.3 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8-1 from local-m2-cache in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.13.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.12;12.0 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.13.0 from central in [default]\n",
      "\torg.bytedeco#artoolkitplus;2.3.1-1.5.6 from central in [default]\n",
      "\torg.bytedeco#artoolkitplus-platform;2.3.1-1.5.6 from central in [default]\n",
      "\torg.bytedeco#ffmpeg;4.4-1.5.6 from central in [default]\n",
      "\torg.bytedeco#ffmpeg-platform;4.4-1.5.6 from central in [default]\n",
      "\torg.bytedeco#flandmark;1.07-1.5.6 from central in [default]\n",
      "\torg.bytedeco#flandmark-platform;1.07-1.5.6 from central in [default]\n",
      "\torg.bytedeco#flycapture;2.13.3.31-1.5.6 from central in [default]\n",
      "\torg.bytedeco#flycapture-platform;2.13.3.31-1.5.6 from central in [default]\n",
      "\torg.bytedeco#javacpp;1.5.6 from central in [default]\n",
      "\torg.bytedeco#javacpp-platform;1.5.6 from central in [default]\n",
      "\torg.bytedeco#javacv;1.5.6 from central in [default]\n",
      "\torg.bytedeco#javacv-platform;1.5.6 from central in [default]\n",
      "\torg.bytedeco#leptonica;1.81.1-1.5.6 from central in [default]\n",
      "\torg.bytedeco#leptonica-platform;1.81.1-1.5.6 from central in [default]\n",
      "\torg.bytedeco#libdc1394;2.2.6-1.5.6 from central in [default]\n",
      "\torg.bytedeco#libdc1394-platform;2.2.6-1.5.6 from central in [default]\n",
      "\torg.bytedeco#libfreenect;0.5.7-1.5.6 from central in [default]\n",
      "\torg.bytedeco#libfreenect-platform;0.5.7-1.5.6 from central in [default]\n",
      "\torg.bytedeco#libfreenect2;0.2.0-1.5.6 from central in [default]\n",
      "\torg.bytedeco#libfreenect2-platform;0.2.0-1.5.6 from central in [default]\n",
      "\torg.bytedeco#librealsense;1.12.4-1.5.6 from central in [default]\n",
      "\torg.bytedeco#librealsense-platform;1.12.4-1.5.6 from central in [default]\n",
      "\torg.bytedeco#librealsense2;2.44.0-1.5.6 from central in [default]\n",
      "\torg.bytedeco#librealsense2-platform;2.44.0-1.5.6 from central in [default]\n",
      "\torg.bytedeco#openblas;0.3.17-1.5.6 from central in [default]\n",
      "\torg.bytedeco#openblas-platform;0.3.17-1.5.6 from central in [default]\n",
      "\torg.bytedeco#opencv;4.5.3-1.5.6 from central in [default]\n",
      "\torg.bytedeco#opencv-platform;4.5.3-1.5.6 from central in [default]\n",
      "\torg.bytedeco#tesseract;4.1.1-1.5.6 from central in [default]\n",
      "\torg.bytedeco#tesseract-platform;4.1.1-1.5.6 from central in [default]\n",
      "\torg.bytedeco#videoinput;0.200-1.5.6 from central in [default]\n",
      "\torg.bytedeco#videoinput-platform;0.200-1.5.6 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from spark-list in [default]\n",
      "\torg.typelevel#cats-core_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#cats-macros_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#jawn-parser_2.12;0.14.2 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from spark-list in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   50  |   34  |   34  |   0   ||   50  |   33  |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9bb052d4-440d-4d8d-adc2-b9edc8132286\n",
      "\tconfs: [default]\n",
      "\t33 artifacts copied, 17 already retrieved (91856kB/76ms)\n",
      "21/10/29 14:34:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-10-29 14:35:20,093 INFO Rikai (callback_service.py:54): Spark callback server started\n",
      "2021-10-29 14:35:20,100 INFO Rikai (callback_service.py:113): Rikai Python CallbackService is registered to SparkSession\n"
     ]
    }
   ],
   "source": [
    "from example import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a8b73bd-5f66-4a0b-a533-36f8e470997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     950|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/tmp/rikai_example/elephants_dream\")\n",
    "df.createOrReplaceTempView(\"elephants_dream\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select count(*) from elephants_dream\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b269c5-7364-4abe-9130-ee424d850d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 10:06:29,594 INFO Rikai (mlflow_registry.py:223): Resolving model mlflow_yolov5_m from mlflow:///yolov5s-model\n",
      "2021-10-28 10:06:30,738 INFO Rikai (base.py:207): Created model inference pandas_udf with name mlflow_yolov5_m_baf9faf4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_tracking_uri = \"sqlite:///mlruns.db\"\n",
    "registered_model_name = f\"yolov5s-model\"\n",
    "\n",
    "spark.conf.set(\"rikai.sql.ml.registry.mlflow.tracking_uri\", mlflow_tracking_uri)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE MODEL mlflow_yolov5_m\n",
    "    PREPROCESSOR 'rikai.contrib.torch.transforms.yolov5.pre_processing'\n",
    "    POSTPROCESSOR 'rikai.contrib.torch.transforms.yolov5.post_processing'\n",
    "    USING 'mlflow:///{registered_model_name}';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20af65ab-bb47-4dab-81f4-27248bb5dfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.inspect_bounding_boxes(img: rikai.types.vision.Image, boxes, labels)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Bounding Boxes on an Image\n",
    "from rikai.types.vision import Image\n",
    "from rikai.spark.types.vision import ImageType\n",
    "from rikai.types.geometry import Box2d\n",
    "from pyspark.sql.functions import udf\n",
    "from PIL import ImageDraw\n",
    "\n",
    "@udf(returnType=ImageType())\n",
    "def inspect_bounding_boxes(img: Image, boxes, labels):\n",
    "    pil_image = img.to_pil()\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    for i in range(len(labels)):\n",
    "        box = boxes[i]\n",
    "        label = labels[i]\n",
    "        bbox = Box2d(box[0], box[1], box[2], box[3])\n",
    "        draw.rectangle(bbox.to_numpy().tolist(), outline=\"green\", width=2)\n",
    "        draw.text([bbox.xmin + 5, bbox.ymin - 10], str(label), fill=\"red\")\n",
    "    return Image.from_pil(pil_image)\n",
    "\n",
    "spark.udf.register(\"inspect_bbox\", inspect_bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3643288d-aa35-4ade-8cf8-b5588d2ac0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.frame(path: str, frame_id: int)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from rikai.types.vision import Image\n",
    "from pyspark.sql.functions import udf\n",
    "from rikai.spark.types.vision import ImageType\n",
    "\n",
    "\n",
    "@udf(returnType=ImageType())\n",
    "def frame(path: str, frame_id: int): \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frame_cnt = 0\n",
    "    img0 = None\n",
    "    while frame_cnt <= frame_id:\n",
    "        ret_val, img0 = cap.read()\n",
    "        if not ret_val:\n",
    "            print(f\"{frame_id} out of bound\")\n",
    "            return\n",
    "        frame_cnt = frame_cnt + 1\n",
    "    cap.release()\n",
    "    img = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "    return Image.from_array(img)\n",
    "\n",
    "spark.udf.register('frame', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9e2e1d-0be4-4afc-bc43-dbcbd21df516",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = spark.sql(\"\"\"\n",
    "from (\n",
    "    from (\n",
    "        select\n",
    "            frame_id,\n",
    "            ML_PREDICT(mlflow_yolov5_m, image) as pred \n",
    "        from elephants_dream\n",
    "    )\n",
    "    select\n",
    "        frame('elephants_dream.mp4', frame_id) as image,\n",
    "        pred\n",
    "    where\n",
    "        size(pred.label_ids) > 1\n",
    ")\n",
    "select\n",
    "    image,\n",
    "    pred,\n",
    "    inspect_bbox(image, pred.boxes, pred.label_ids) as bbox\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8bf4afd-9157-4a95-959f-75135b63eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/29 09:23:55 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 584, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:103)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\t... 3 more\n",
      "21/10/29 09:23:55 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/10/29 09:23:55 ERROR ArrowPythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 584, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\t... 17 more\n",
      "21/10/29 09:23:55 ERROR ArrowPythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/10/29 09:23:55 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 584, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/10/29 09:23:55 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/10/29 09:23:55 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 13)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/10/29 09:23:55 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 13) (192.168.31.58 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "\n",
      "21/10/29 09:23:55 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n    arg_offsets, udf = read_single_udf(\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6d/q8_1m19n1jzg6_3lghwylhpc0000gp/T/ipykernel_25834/638026851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 336, in read_udfs\n    arg_offsets, udf = read_single_udf(\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_identity' on <module 'rikai.spark.sql.codegen.pytorch' from '/Users/da/github/eto-ai/rikai/python/rikai/spark/sql/codegen/pytorch.py'>\n"
     ]
    }
   ],
   "source": [
    "rows = predicts.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcc8c1-0b82-49fa-b219-3780f9269910",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0].image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e22c1-e8d0-4d93-99a0-445542a3c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0].bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7385c-d393-49b8-8306-a458f9f9ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0].pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fab95c0-cef7-4b37-b05d-2fa332267c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| label|\n",
      "+---+------+\n",
      "| 74|remote|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from parquet.`/tmp/rikai_example/coco_labels`\n",
    "where id = 74\n",
    "order by id asc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98026c83-f6a3-4b03-86ec-83fba572c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae537f2a-6166-4ba3-95aa-c36cf19d47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|frame_id|               image|\n",
      "+--------+--------------------+\n",
      "|     378|[63 6E 75 6D 70 7...|\n",
      "|     392|[63 6E 75 6D 70 7...|\n",
      "|      51|[63 6E 75 6D 70 7...|\n",
      "|     425|[63 6E 75 6D 70 7...|\n",
      "|     407|[63 6E 75 6D 70 7...|\n",
      "|      64|[63 6E 75 6D 70 7...|\n",
      "|      38|[63 6E 75 6D 70 7...|\n",
      "|      31|[63 6E 75 6D 70 7...|\n",
      "|     477|[63 6E 75 6D 70 7...|\n",
      "|     249|[63 6E 75 6D 70 7...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from elephants_dream\n",
    "\"\"\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed395d-4bbb-426d-b72a-08a01a87faf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
